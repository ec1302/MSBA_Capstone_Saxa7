{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignoring warnings (many warnings come from the FairLearn package)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading in the data and getting an overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35000, 21)\n",
      "Index(['action_taken', 'preapproval', 'property_value', 'loan_purpose',\n",
      "       'lien_status', 'reverse_mortgage', 'open-end_line_of_credit',\n",
      "       'business_or_commercial_purpose', 'loan_amount', 'loan_to_value_ratio',\n",
      "       'loan_term', 'negative_amortization', 'occupancy_type', 'income',\n",
      "       'debt_to_income_ratio', 'applicant_sex', 'conforming_loan_limit',\n",
      "       'derived_loan_product_type', 'derived_dwelling_category',\n",
      "       'derived_race', 'applicant_age_above_62'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Starting with 35k observations that have NA values filled from KNN imputation\n",
    "df = pd.read_csv('na_filled_df.csv').sample(n=35000, random_state=2025)\n",
    "print(df.shape)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['applicant_sex', 'conforming_loan_limit', 'derived_loan_product_type',\n",
      "       'derived_dwelling_category', 'derived_race', 'applicant_age_above_62'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Finding categorical variables\n",
    "string_columns = df.select_dtypes(include=['object']).columns\n",
    "string_df = df[string_columns].copy()\n",
    "print(string_columns)\n",
    "sensitive_features = df[['applicant_sex', 'derived_race', 'applicant_age_above_62']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure these are strings and not mixed data types\n",
    "df['applicant_sex'] = df['applicant_sex'].astype(str)\n",
    "df['applicant_age_above_62'] = df['applicant_age_above_62'].astype(str)\n",
    "df['derived_race'] = df['derived_race'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy variables for the random forest model\n",
    "string_columns = df.select_dtypes(include=['object']).columns\n",
    "df_dummies = pd.get_dummies(df, columns=string_columns, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data\n",
    "y = df_dummies['action_taken']\n",
    "X = df_dummies.drop(columns='action_taken')\n",
    "# Inlcuding the sensitive features also to use later in Fairlearn\n",
    "sensitive_features_data = df[['applicant_sex', 'applicant_age_above_62', 'derived_race']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=2023)\n",
    "sensitive_features_train, sensitive_features_test = train_test_split(sensitive_features_data, test_size=0.25, random_state=2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.reductions import EqualizedOdds, ExponentiatedGradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Emerson\\Desktop\\MSBA\\Capstone\\Code\\MSBA_Capstone_Saxa7\\Models\\random_forest_inprocessing.ipynb Cell 14\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Emerson/Desktop/MSBA/Capstone/Code/MSBA_Capstone_Saxa7/Models/random_forest_inprocessing.ipynb#Y101sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m classifier \u001b[39m=\u001b[39m RandomForestClassifier(n_estimators\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m2023\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Emerson/Desktop/MSBA/Capstone/Code/MSBA_Capstone_Saxa7/Models/random_forest_inprocessing.ipynb#Y101sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m mitigator \u001b[39m=\u001b[39m ExponentiatedGradient(classifier, constraint)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Emerson/Desktop/MSBA/Capstone/Code/MSBA_Capstone_Saxa7/Models/random_forest_inprocessing.ipynb#Y101sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m mitigator\u001b[39m.\u001b[39;49mfit(X_train, y_train, sensitive_features\u001b[39m=\u001b[39;49msensitive_features_train[\u001b[39m'\u001b[39;49m\u001b[39mderived_race\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Emerson/Desktop/MSBA/Capstone/Code/MSBA_Capstone_Saxa7/Models/random_forest_inprocessing.ipynb#Y101sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m y_pred_mitigated \u001b[39m=\u001b[39m mitigator\u001b[39m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\Users\\Emerson\\Desktop\\MSBA\\Capstone\\Code\\MSBA_Capstone_Saxa7\\capstone_env\\Lib\\site-packages\\fairlearn\\reductions\\_exponentiated_gradient\\exponentiated_gradient.py:168\u001b[0m, in \u001b[0;36mExponentiatedGradient.fit\u001b[1;34m(self, X, y, **kwargs)\u001b[0m\n\u001b[0;32m    165\u001b[0m lambda_EG \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlambda_vecs_EG_\u001b[39m.\u001b[39mmean(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    167\u001b[0m \u001b[39m# select classifier according to best_h method\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m h, h_idx \u001b[39m=\u001b[39m lagrangian\u001b[39m.\u001b[39;49mbest_h(lambda_vec)\n\u001b[0;32m    170\u001b[0m \u001b[39mif\u001b[39;00m t \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    171\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnu \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Emerson\\Desktop\\MSBA\\Capstone\\Code\\MSBA_Capstone_Saxa7\\capstone_env\\Lib\\site-packages\\fairlearn\\reductions\\_exponentiated_gradient\\_lagrangian.py:242\u001b[0m, in \u001b[0;36m_Lagrangian.best_h\u001b[1;34m(self, lambda_vec)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbest_h\u001b[39m(\u001b[39mself\u001b[39m, lambda_vec):\n\u001b[0;32m    237\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Solve the best-response problem.\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \n\u001b[0;32m    239\u001b[0m \u001b[39m    Returns the classifier that solves the best-response problem for\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[39m    the vector of Lagrange multipliers `lambda_vec`.\u001b[39;00m\n\u001b[0;32m    241\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 242\u001b[0m     classifier \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_oracle(lambda_vec)\n\u001b[0;32m    244\u001b[0m     h \u001b[39m=\u001b[39m _PredictorAsCallable(classifier)\n\u001b[0;32m    246\u001b[0m     h_error \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39mgamma(h)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Emerson\\Desktop\\MSBA\\Capstone\\Code\\MSBA_Capstone_Saxa7\\capstone_env\\Lib\\site-packages\\fairlearn\\reductions\\_exponentiated_gradient\\_lagrangian.py:230\u001b[0m, in \u001b[0;36m_Lagrangian._call_oracle\u001b[1;34m(self, lambda_vec)\u001b[0m\n\u001b[0;32m    227\u001b[0m     estimator \u001b[39m=\u001b[39m clone(estimator\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimator, safe\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    229\u001b[0m oracle_call_start_time \u001b[39m=\u001b[39m time()\n\u001b[1;32m--> 230\u001b[0m estimator\u001b[39m.\u001b[39;49mfit(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconstraints\u001b[39m.\u001b[39;49mX, redY, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m{\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msample_weight_name: redW})\n\u001b[0;32m    231\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moracle_execution_times\u001b[39m.\u001b[39mappend(time() \u001b[39m-\u001b[39m oracle_call_start_time)\n\u001b[0;32m    232\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_oracle_calls \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Emerson\\Desktop\\MSBA\\Capstone\\Code\\MSBA_Capstone_Saxa7\\capstone_env\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Emerson\\Desktop\\MSBA\\Capstone\\Code\\MSBA_Capstone_Saxa7\\capstone_env\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:456\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    445\u001b[0m trees \u001b[39m=\u001b[39m [\n\u001b[0;32m    446\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(append\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[0;32m    447\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    448\u001b[0m ]\n\u001b[0;32m    450\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 456\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n\u001b[0;32m    457\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[0;32m    458\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    459\u001b[0m     prefer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthreads\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    460\u001b[0m )(\n\u001b[0;32m    461\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[0;32m    462\u001b[0m         t,\n\u001b[0;32m    463\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbootstrap,\n\u001b[0;32m    464\u001b[0m         X,\n\u001b[0;32m    465\u001b[0m         y,\n\u001b[0;32m    466\u001b[0m         sample_weight,\n\u001b[0;32m    467\u001b[0m         i,\n\u001b[0;32m    468\u001b[0m         \u001b[39mlen\u001b[39;49m(trees),\n\u001b[0;32m    469\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    470\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[0;32m    471\u001b[0m         n_samples_bootstrap\u001b[39m=\u001b[39;49mn_samples_bootstrap,\n\u001b[0;32m    472\u001b[0m     )\n\u001b[0;32m    473\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, t \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(trees)\n\u001b[0;32m    474\u001b[0m )\n\u001b[0;32m    476\u001b[0m \u001b[39m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    477\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Users\\Emerson\\Desktop\\MSBA\\Capstone\\Code\\MSBA_Capstone_Saxa7\\capstone_env\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\Emerson\\Desktop\\MSBA\\Capstone\\Code\\MSBA_Capstone_Saxa7\\capstone_env\\Lib\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39m(output)\n\u001b[0;32m   1865\u001b[0m \u001b[39m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[39m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[39m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[39m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[39m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\Emerson\\Desktop\\MSBA\\Capstone\\Code\\MSBA_Capstone_Saxa7\\capstone_env\\Lib\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1793\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_completed_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\Emerson\\Desktop\\MSBA\\Capstone\\Code\\MSBA_Capstone_Saxa7\\capstone_env\\Lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Emerson\\Desktop\\MSBA\\Capstone\\Code\\MSBA_Capstone_Saxa7\\capstone_env\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:188\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[39melif\u001b[39;00m class_weight \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbalanced_subsample\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    186\u001b[0m         curr_sample_weight \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m compute_sample_weight(\u001b[39m\"\u001b[39m\u001b[39mbalanced\u001b[39m\u001b[39m\"\u001b[39m, y, indices\u001b[39m=\u001b[39mindices)\n\u001b[1;32m--> 188\u001b[0m     tree\u001b[39m.\u001b[39;49mfit(X, y, sample_weight\u001b[39m=\u001b[39;49mcurr_sample_weight, check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    189\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     tree\u001b[39m.\u001b[39mfit(X, y, sample_weight\u001b[39m=\u001b[39msample_weight, check_input\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Emerson\\Desktop\\MSBA\\Capstone\\Code\\MSBA_Capstone_Saxa7\\capstone_env\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Emerson\\Desktop\\MSBA\\Capstone\\Code\\MSBA_Capstone_Saxa7\\capstone_env\\Lib\\site-packages\\sklearn\\tree\\_classes.py:959\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[39m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    929\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    930\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    931\u001b[0m \n\u001b[0;32m    932\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    957\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 959\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_fit(\n\u001b[0;32m    960\u001b[0m         X,\n\u001b[0;32m    961\u001b[0m         y,\n\u001b[0;32m    962\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    963\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[0;32m    964\u001b[0m     )\n\u001b[0;32m    965\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Emerson\\Desktop\\MSBA\\Capstone\\Code\\MSBA_Capstone_Saxa7\\capstone_env\\Lib\\site-packages\\sklearn\\tree\\_classes.py:443\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    433\u001b[0m     builder \u001b[39m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    434\u001b[0m         splitter,\n\u001b[0;32m    435\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    441\u001b[0m     )\n\u001b[1;32m--> 443\u001b[0m builder\u001b[39m.\u001b[39;49mbuild(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_, X, y, sample_weight, missing_values_in_feature_mask)\n\u001b[0;32m    445\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[0;32m    446\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "np.random.seed(2023)\n",
    "\n",
    "constraint = EqualizedOdds()\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=2023)\n",
    "mitigator = ExponentiatedGradient(classifier, constraint)\n",
    "mitigator.fit(X_train, y_train, sensitive_features=sensitive_features_train['derived_race'])\n",
    "\n",
    "y_pred_mitigated = mitigator.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Printing metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "# Create a custom color map\n",
    "cmap = sns.light_palette(\"green\", as_cmap=True)\n",
    "\n",
    "# Plot the confusion matrix using seaborn's heatmap\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=cmap, cbar=False,\n",
    "            xticklabels=['Predicted 0', 'Predicted 1'],\n",
    "            yticklabels=['Actual 0', 'Actual 1'])\n",
    "\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the ROC curve\n",
    "y_prob = rf0.predict_proba(X_test)[:,1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rf0.feature_importances_\n",
    "sorted_indices = np.argsort(importances)[::-1]\n",
    "sorted_features = [X.columns[idx] for idx in sorted_indices]\n",
    "\n",
    "sorted_features_with_importance = []\n",
    "for i, idx in enumerate(sorted_indices):\n",
    "    feature_name = X.columns[idx]\n",
    "    importance_value = importances[idx]\n",
    "    print(f\"{i + 1}. {feature_name} ({importance_value})\")\n",
    "    sorted_features_with_importance.append((feature_name, importance_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking into metrics with Microsoft Fairlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.metrics import MetricFrame\n",
    "from fairlearn.metrics import selection_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy by race\n",
    "mf = MetricFrame(metrics=accuracy_score, y_true=y_test, y_pred=y_pred, sensitive_features=sensitive_features_test['derived_race'])\n",
    "print(mf.overall)\n",
    "mf.by_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selection rate is the percentage of the population that is labeled '1'\n",
    "sr = MetricFrame(metrics=selection_rate, y_true=y_test, y_pred=y_pred, sensitive_features=sensitive_features_test['derived_race'])\n",
    "print(sr.overall)\n",
    "sr.by_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually defining positives and negatives\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "# Define False Positive Rate\n",
    "def false_positive_rate(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return fp / (fp + tn)\n",
    "\n",
    "# Define False Negative Rate\n",
    "def false_negative_rate(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return fn / (fn + tp)\n",
    "\n",
    "# Define Selection Rate\n",
    "def selection_rate(y_true, y_pred):\n",
    "    return sum(y_pred) / len(y_pred)\n",
    "\n",
    "# Define Count (this is simply the length of y_pred)\n",
    "def count(y_true, y_pred):\n",
    "    return len(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applicant sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    \"accuracy\": accuracy_score,\n",
    "    \"precision\": precision_score,\n",
    "    \"recall\": recall_score,\n",
    "    \"F1 score\": f1_score,\n",
    "    \"false positive rate\": false_positive_rate,\n",
    "    \"false negative rate\": false_negative_rate,\n",
    "    \"selection rate\": selection_rate,\n",
    "    \"count\": count,\n",
    "}\n",
    "metric_frame = MetricFrame(\n",
    "    metrics=metrics, y_true=y_test, y_pred=y_pred, sensitive_features=sensitive_features_test['applicant_sex'])\n",
    "\n",
    "metric_frame.by_group.plot.bar(\n",
    "    subplots=True,\n",
    "    layout=[3, 3],\n",
    "    legend=False,\n",
    "    figsize=[12, 8],\n",
    "    title=\"Show all metrics\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same info as the graph above but in table format\n",
    "\n",
    "# Create a rounded table\n",
    "rounded_table = metric_frame.by_group.round(3)\n",
    "\n",
    "# Display the table\n",
    "print(rounded_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applicant race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_frame = MetricFrame(\n",
    "    metrics=metrics, y_true=y_test, y_pred=y_pred, sensitive_features=sensitive_features_test['derived_race'])\n",
    "\n",
    "metric_frame.by_group.plot.bar(\n",
    "    subplots=True,\n",
    "    layout=[3, 3],\n",
    "    legend=False,\n",
    "    figsize=[12, 8],\n",
    "    title=\"Show all metrics\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same info as the graph above but in table format\n",
    "\n",
    "# Create a rounded table\n",
    "rounded_table = metric_frame.by_group.round(3)\n",
    "\n",
    "# Display the table\n",
    "print(rounded_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applicant age above 62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_frame = MetricFrame(\n",
    "    metrics=metrics, y_true=y_test, \n",
    "    y_pred=y_pred, sensitive_features=sensitive_features_test['applicant_age_above_62'])\n",
    "\n",
    "metric_frame.by_group.plot.bar(\n",
    "    subplots=True,\n",
    "    layout=[3, 3],\n",
    "    legend=False,\n",
    "    figsize=[12, 8],\n",
    "    title=\"Show all metrics\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same info as the graph above but in table format\n",
    "\n",
    "# Create a rounded table\n",
    "rounded_table = metric_frame.by_group.round(3)\n",
    "\n",
    "# Display the table\n",
    "print(rounded_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # Getting errors for race and sex metrics. It seems that some groups are too small and causing errors.\n",
    "print(df['derived_race'].value_counts()/len(df)*100)\n",
    "sns.countplot(x='derived_race', data=df) \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the objective function with Exponentiated Gradient mitigation  using Demographic Parity as the objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.reductions import EqualizedOdds, ExponentiatedGradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2023)\n",
    "\n",
    "constraint = EqualizedOdds()\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=2023)\n",
    "mitigator = ExponentiatedGradient(classifier, constraint)\n",
    "mitigator.fit(X_train, y_train, sensitive_features=sensitive_features_train['applicant_sex'])\n",
    "\n",
    "y_pred_mitigated = mitigator.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing mitigated to original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the metrics\n",
    "metrics_list = ['false_positive_rate', 'false_negative_rate', 'f1_score']\n",
    "\n",
    "# Initialize an empty dictionary to hold the metrics\n",
    "metrics_dict = {\n",
    "    'FPR Male': [],\n",
    "    'FPR Female': [],\n",
    "    'FPR Other/NA': [],\n",
    "    'FNR Male': [],\n",
    "    'FNR Female': [],\n",
    "    'FNR Other/NA': [],\n",
    "    'F1 Male': [],\n",
    "    'F1 Female': [],\n",
    "    'F1 Other/NA': []\n",
    "}\n",
    "\n",
    "# Populate the dictionary with the metric values for mitigated and original models\n",
    "for metric_name in metrics_list:\n",
    "    for group in ['Male', 'Female', 'Other/NA']:\n",
    "        mitigated_metric = metric_frame_mitigated.by_group[metric_name][group].round(3) if metric_name in metric_frame_mitigated.by_group else None\n",
    "        original_metric = metric_frame_original.by_group[metric_name][group].round(3) if metric_name in metric_frame_original.by_group else None\n",
    "        key_suffix = 'FPR' if metric_name == 'false_positive_rate' else 'FNR' if metric_name == 'false_negative_rate' else 'F1'\n",
    "        metrics_dict[f'{key_suffix} {group}'].append(mitigated_metric)\n",
    "        metrics_dict[f'{key_suffix} {group}'].append(original_metric)\n",
    "\n",
    "# Create the DataFrame\n",
    "metrics_df = pd.DataFrame(metrics_dict, index=['Mitigated', 'Original'])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the metrics\n",
    "metrics = {\n",
    "    'false_positive_rate': false_positive_rate,\n",
    "    'false_negative_rate': false_negative_rate,\n",
    "    'f1_score': f1_score,\n",
    "}\n",
    "\n",
    "# Calculate metrics for the mitigated model\n",
    "metric_frame_mitigated = MetricFrame(metrics=metrics,\n",
    "                                     y_true=y_test,\n",
    "                                     y_pred=y_pred_mitigated,\n",
    "                                     sensitive_features=sensitive_features_test['applicant_sex'])\n",
    "\n",
    "# Calculate metrics for the original model\n",
    "metric_frame_original = MetricFrame(metrics=metrics,\n",
    "                                    y_true=y_test,\n",
    "                                    y_pred=y_pred,\n",
    "                                    sensitive_features=sensitive_features_test['applicant_sex'])\n",
    "\n",
    "# Create a dictionary to hold the overall metrics for mitigated and original models\n",
    "overall_metrics_dict = {\n",
    "    'FPR': [\n",
    "        metric_frame_mitigated.overall['false_positive_rate'].round(3),\n",
    "        metric_frame_original.overall['false_positive_rate'].round(3)\n",
    "    ],\n",
    "    'FNR': [\n",
    "        metric_frame_mitigated.overall['false_negative_rate'].round(3),\n",
    "        metric_frame_original.overall['false_negative_rate'].round(3)\n",
    "    ],\n",
    "    'F1 Score': [\n",
    "        metric_frame_mitigated.overall['f1_score'].round(3),\n",
    "        metric_frame_original.overall['f1_score'].round(3)\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create the DataFrame with 'Mitigated' and 'Original' as the index\n",
    "overall_metrics_df = pd.DataFrame(overall_metrics_dict, index=['Mitigated', 'Original'])\n",
    "\n",
    "# Display the DataFrame\n",
    "print('Overall metrics')\n",
    "print(overall_metrics_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "\n",
    "# Setup the postprocessed model\n",
    "postprocessed_model = ThresholdOptimizer(\n",
    "    estimator= rf0,\n",
    "    constraints='false_negative_rate_parity',\n",
    "    objective='balanced_accuracy_score',\n",
    "    prefit=True  # Since the estimator is already fitted\n",
    ")\n",
    "\n",
    "# Fit the ThresholdOptimizer\n",
    "# Remember to use the test set to avoid data leakage\n",
    "postprocessed_model.fit(X_test, y_test, sensitive_features=sensitive_features_test['applicant_sex'])\n",
    "\n",
    "# Generate mitigated predictions\n",
    "y_pred_postprocessed = postprocessed_model.predict(X_test, sensitive_features=sensitive_features_test['applicant_sex'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the metrics\n",
    "metrics = {\n",
    "    'false_positive_rate': false_positive_rate,\n",
    "    'false_negative_rate': false_negative_rate,\n",
    "    'f1_score': f1_score\n",
    "}\n",
    "\n",
    "# Calculate metrics for the original model\n",
    "metric_frame_original = MetricFrame(\n",
    "    metrics=metrics,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred,  # Predictions from the original model\n",
    "    sensitive_features=sensitive_features_test['applicant_sex']\n",
    ")\n",
    "\n",
    "# Calculate metrics for the post-processed model\n",
    "metric_frame_mitigated = MetricFrame(\n",
    "    metrics=metrics,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_postprocessed,  # Predictions from the post-processed model\n",
    "    sensitive_features=sensitive_features_test['applicant_sex']\n",
    ")\n",
    "\n",
    "# Initialize an empty dictionary to hold the metrics\n",
    "metrics_dict = {\n",
    "    'FPR Male': [],\n",
    "    'FPR Female': [],\n",
    "    'FPR Other/NA': [],\n",
    "    'FNR Male': [],\n",
    "    'FNR Female': [],\n",
    "    'FNR Other/NA': [],\n",
    "    'F1 Male': [],\n",
    "    'F1 Female': [],\n",
    "    'F1 Other/NA': []\n",
    "}\n",
    "\n",
    "# Populate the dictionary with the metric values for mitigated and original models\n",
    "groups = ['Male', 'Female', 'Other/NA']  # Adjust these as per your sensitive attribute categories\n",
    "for metric_name in metrics:\n",
    "    for group in groups:\n",
    "        original_metric = metric_frame_original.by_group[metric_name][group].round(3) if group in metric_frame_original.by_group[metric_name] else None\n",
    "        mitigated_metric = metric_frame_mitigated.by_group[metric_name][group].round(3) if group in metric_frame_mitigated.by_group[metric_name] else None\n",
    "        \n",
    "        key_suffix = 'FPR' if metric_name == 'false_positive_rate' else 'FNR' if metric_name == 'false_negative_rate' else 'F1'\n",
    "        metrics_dict[f'{key_suffix} {group}'].append(original_metric)\n",
    "        metrics_dict[f'{key_suffix} {group}'].append(mitigated_metric)\n",
    "\n",
    "# Create the DataFrame\n",
    "metrics_df = pd.DataFrame(metrics_dict, index=['Original', 'Mitigated'])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(metrics_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
