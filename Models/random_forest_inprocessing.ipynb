{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignoring warnings (many warnings come from the FairLearn package)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading in the data and getting an overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35000, 21)\n",
      "Index(['action_taken', 'preapproval', 'property_value', 'loan_purpose',\n",
      "       'lien_status', 'reverse_mortgage', 'open-end_line_of_credit',\n",
      "       'business_or_commercial_purpose', 'loan_amount', 'loan_to_value_ratio',\n",
      "       'loan_term', 'negative_amortization', 'occupancy_type', 'income',\n",
      "       'debt_to_income_ratio', 'applicant_sex', 'conforming_loan_limit',\n",
      "       'derived_loan_product_type', 'derived_dwelling_category',\n",
      "       'derived_race', 'applicant_age_above_62'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Starting with 35k observations that have NA values filled from KNN imputation\n",
    "df = pd.read_csv('na_filled_df.csv').sample(n=35000, random_state=2025)\n",
    "print(df.shape)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['applicant_sex', 'conforming_loan_limit', 'derived_loan_product_type',\n",
      "       'derived_dwelling_category', 'derived_race', 'applicant_age_above_62'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Finding categorical variables\n",
    "string_columns = df.select_dtypes(include=['object']).columns\n",
    "string_df = df[string_columns].copy()\n",
    "print(string_columns)\n",
    "sensitive_features = df[['applicant_sex', 'derived_race', 'applicant_age_above_62']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure these are strings and not mixed data types\n",
    "df['applicant_sex'] = df['applicant_sex'].astype(str)\n",
    "df['applicant_age_above_62'] = df['applicant_age_above_62'].astype(str)\n",
    "df['derived_race'] = df['derived_race'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy variables for the random forest model\n",
    "string_columns = df.select_dtypes(include=['object']).columns\n",
    "df_dummies = pd.get_dummies(df, columns=string_columns, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data\n",
    "y = df_dummies['action_taken']\n",
    "X = df_dummies.drop(columns='action_taken')\n",
    "# Inlcuding the sensitive features also to use later in Fairlearn\n",
    "sensitive_features_data = df[['applicant_sex', 'applicant_age_above_62', 'derived_race']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=2023)\n",
    "sensitive_features_train, sensitive_features_test = train_test_split(sensitive_features_data, test_size=0.25, random_state=2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.reductions import EqualizedOdds, ExponentiatedGradient\n",
    "\n",
    "np.random.seed(2023)\n",
    "\n",
    "constraint = EqualizedOdds()\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=2023)\n",
    "mitigator = ExponentiatedGradient(classifier, constraint)\n",
    "mitigator.fit(X_train, y_train, sensitive_features=sensitive_features_train['applicant_sex'])\n",
    "\n",
    "y_pred_mitigated = mitigator.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Printing metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "# Create a custom color map\n",
    "cmap = sns.light_palette(\"green\", as_cmap=True)\n",
    "\n",
    "# Plot the confusion matrix using seaborn's heatmap\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=cmap, cbar=False,\n",
    "            xticklabels=['Predicted 0', 'Predicted 1'],\n",
    "            yticklabels=['Actual 0', 'Actual 1'])\n",
    "\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the ROC curve\n",
    "y_prob = rf0.predict_proba(X_test)[:,1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rf0.feature_importances_\n",
    "sorted_indices = np.argsort(importances)[::-1]\n",
    "sorted_features = [X.columns[idx] for idx in sorted_indices]\n",
    "\n",
    "sorted_features_with_importance = []\n",
    "for i, idx in enumerate(sorted_indices):\n",
    "    feature_name = X.columns[idx]\n",
    "    importance_value = importances[idx]\n",
    "    print(f\"{i + 1}. {feature_name} ({importance_value})\")\n",
    "    sorted_features_with_importance.append((feature_name, importance_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking into metrics with Microsoft Fairlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.metrics import MetricFrame\n",
    "from fairlearn.metrics import selection_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy by race\n",
    "mf = MetricFrame(metrics=accuracy_score, y_true=y_test, y_pred=y_pred, sensitive_features=sensitive_features_test['derived_race'])\n",
    "print(mf.overall)\n",
    "mf.by_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selection rate is the percentage of the population that is labeled '1'\n",
    "sr = MetricFrame(metrics=selection_rate, y_true=y_test, y_pred=y_pred, sensitive_features=sensitive_features_test['derived_race'])\n",
    "print(sr.overall)\n",
    "sr.by_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually defining positives and negatives\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "# Define False Positive Rate\n",
    "def false_positive_rate(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return fp / (fp + tn)\n",
    "\n",
    "# Define False Negative Rate\n",
    "def false_negative_rate(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return fn / (fn + tp)\n",
    "\n",
    "# Define Selection Rate\n",
    "def selection_rate(y_true, y_pred):\n",
    "    return sum(y_pred) / len(y_pred)\n",
    "\n",
    "# Define Count (this is simply the length of y_pred)\n",
    "def count(y_true, y_pred):\n",
    "    return len(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applicant sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    \"accuracy\": accuracy_score,\n",
    "    \"precision\": precision_score,\n",
    "    \"recall\": recall_score,\n",
    "    \"F1 score\": f1_score,\n",
    "    \"false positive rate\": false_positive_rate,\n",
    "    \"false negative rate\": false_negative_rate,\n",
    "    \"selection rate\": selection_rate,\n",
    "    \"count\": count,\n",
    "}\n",
    "metric_frame = MetricFrame(\n",
    "    metrics=metrics, y_true=y_test, y_pred=y_pred, sensitive_features=sensitive_features_test['applicant_sex'])\n",
    "\n",
    "metric_frame.by_group.plot.bar(\n",
    "    subplots=True,\n",
    "    layout=[3, 3],\n",
    "    legend=False,\n",
    "    figsize=[12, 8],\n",
    "    title=\"Show all metrics\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same info as the graph above but in table format\n",
    "\n",
    "# Create a rounded table\n",
    "rounded_table = metric_frame.by_group.round(3)\n",
    "\n",
    "# Display the table\n",
    "print(rounded_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applicant race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_frame = MetricFrame(\n",
    "    metrics=metrics, y_true=y_test, y_pred=y_pred, sensitive_features=sensitive_features_test['derived_race'])\n",
    "\n",
    "metric_frame.by_group.plot.bar(\n",
    "    subplots=True,\n",
    "    layout=[3, 3],\n",
    "    legend=False,\n",
    "    figsize=[12, 8],\n",
    "    title=\"Show all metrics\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same info as the graph above but in table format\n",
    "\n",
    "# Create a rounded table\n",
    "rounded_table = metric_frame.by_group.round(3)\n",
    "\n",
    "# Display the table\n",
    "print(rounded_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applicant age above 62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_frame = MetricFrame(\n",
    "    metrics=metrics, y_true=y_test, \n",
    "    y_pred=y_pred, sensitive_features=sensitive_features_test['applicant_age_above_62'])\n",
    "\n",
    "metric_frame.by_group.plot.bar(\n",
    "    subplots=True,\n",
    "    layout=[3, 3],\n",
    "    legend=False,\n",
    "    figsize=[12, 8],\n",
    "    title=\"Show all metrics\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same info as the graph above but in table format\n",
    "\n",
    "# Create a rounded table\n",
    "rounded_table = metric_frame.by_group.round(3)\n",
    "\n",
    "# Display the table\n",
    "print(rounded_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # Getting errors for race and sex metrics. It seems that some groups are too small and causing errors.\n",
    "print(df['derived_race'].value_counts()/len(df)*100)\n",
    "sns.countplot(x='derived_race', data=df) \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the objective function with Exponentiated Gradient mitigation  using Demographic Parity as the objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.reductions import EqualizedOdds, ExponentiatedGradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2023)\n",
    "\n",
    "constraint = EqualizedOdds()\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=2023)\n",
    "mitigator = ExponentiatedGradient(classifier, constraint)\n",
    "mitigator.fit(X_train, y_train, sensitive_features=sensitive_features_train['applicant_sex'])\n",
    "\n",
    "y_pred_mitigated = mitigator.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing mitigated to original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the metrics\n",
    "metrics_list = ['false_positive_rate', 'false_negative_rate', 'f1_score']\n",
    "\n",
    "# Initialize an empty dictionary to hold the metrics\n",
    "metrics_dict = {\n",
    "    'FPR Male': [],\n",
    "    'FPR Female': [],\n",
    "    'FPR Other/NA': [],\n",
    "    'FNR Male': [],\n",
    "    'FNR Female': [],\n",
    "    'FNR Other/NA': [],\n",
    "    'F1 Male': [],\n",
    "    'F1 Female': [],\n",
    "    'F1 Other/NA': []\n",
    "}\n",
    "\n",
    "# Populate the dictionary with the metric values for mitigated and original models\n",
    "for metric_name in metrics_list:\n",
    "    for group in ['Male', 'Female', 'Other/NA']:\n",
    "        mitigated_metric = metric_frame_mitigated.by_group[metric_name][group].round(3) if metric_name in metric_frame_mitigated.by_group else None\n",
    "        original_metric = metric_frame_original.by_group[metric_name][group].round(3) if metric_name in metric_frame_original.by_group else None\n",
    "        key_suffix = 'FPR' if metric_name == 'false_positive_rate' else 'FNR' if metric_name == 'false_negative_rate' else 'F1'\n",
    "        metrics_dict[f'{key_suffix} {group}'].append(mitigated_metric)\n",
    "        metrics_dict[f'{key_suffix} {group}'].append(original_metric)\n",
    "\n",
    "# Create the DataFrame\n",
    "metrics_df = pd.DataFrame(metrics_dict, index=['Mitigated', 'Original'])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the metrics\n",
    "metrics = {\n",
    "    'false_positive_rate': false_positive_rate,\n",
    "    'false_negative_rate': false_negative_rate,\n",
    "    'f1_score': f1_score,\n",
    "}\n",
    "\n",
    "# Calculate metrics for the mitigated model\n",
    "metric_frame_mitigated = MetricFrame(metrics=metrics,\n",
    "                                     y_true=y_test,\n",
    "                                     y_pred=y_pred_mitigated,\n",
    "                                     sensitive_features=sensitive_features_test['applicant_sex'])\n",
    "\n",
    "# Calculate metrics for the original model\n",
    "metric_frame_original = MetricFrame(metrics=metrics,\n",
    "                                    y_true=y_test,\n",
    "                                    y_pred=y_pred,\n",
    "                                    sensitive_features=sensitive_features_test['applicant_sex'])\n",
    "\n",
    "# Create a dictionary to hold the overall metrics for mitigated and original models\n",
    "overall_metrics_dict = {\n",
    "    'FPR': [\n",
    "        metric_frame_mitigated.overall['false_positive_rate'].round(3),\n",
    "        metric_frame_original.overall['false_positive_rate'].round(3)\n",
    "    ],\n",
    "    'FNR': [\n",
    "        metric_frame_mitigated.overall['false_negative_rate'].round(3),\n",
    "        metric_frame_original.overall['false_negative_rate'].round(3)\n",
    "    ],\n",
    "    'F1 Score': [\n",
    "        metric_frame_mitigated.overall['f1_score'].round(3),\n",
    "        metric_frame_original.overall['f1_score'].round(3)\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create the DataFrame with 'Mitigated' and 'Original' as the index\n",
    "overall_metrics_df = pd.DataFrame(overall_metrics_dict, index=['Mitigated', 'Original'])\n",
    "\n",
    "# Display the DataFrame\n",
    "print('Overall metrics')\n",
    "print(overall_metrics_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "\n",
    "# Setup the postprocessed model\n",
    "postprocessed_model = ThresholdOptimizer(\n",
    "    estimator= rf0,\n",
    "    constraints='false_negative_rate_parity',\n",
    "    objective='balanced_accuracy_score',\n",
    "    prefit=True  # Since the estimator is already fitted\n",
    ")\n",
    "\n",
    "# Fit the ThresholdOptimizer\n",
    "# Remember to use the test set to avoid data leakage\n",
    "postprocessed_model.fit(X_test, y_test, sensitive_features=sensitive_features_test['applicant_sex'])\n",
    "\n",
    "# Generate mitigated predictions\n",
    "y_pred_postprocessed = postprocessed_model.predict(X_test, sensitive_features=sensitive_features_test['applicant_sex'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the metrics\n",
    "metrics = {\n",
    "    'false_positive_rate': false_positive_rate,\n",
    "    'false_negative_rate': false_negative_rate,\n",
    "    'f1_score': f1_score\n",
    "}\n",
    "\n",
    "# Calculate metrics for the original model\n",
    "metric_frame_original = MetricFrame(\n",
    "    metrics=metrics,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred,  # Predictions from the original model\n",
    "    sensitive_features=sensitive_features_test['applicant_sex']\n",
    ")\n",
    "\n",
    "# Calculate metrics for the post-processed model\n",
    "metric_frame_mitigated = MetricFrame(\n",
    "    metrics=metrics,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_postprocessed,  # Predictions from the post-processed model\n",
    "    sensitive_features=sensitive_features_test['applicant_sex']\n",
    ")\n",
    "\n",
    "# Initialize an empty dictionary to hold the metrics\n",
    "metrics_dict = {\n",
    "    'FPR Male': [],\n",
    "    'FPR Female': [],\n",
    "    'FPR Other/NA': [],\n",
    "    'FNR Male': [],\n",
    "    'FNR Female': [],\n",
    "    'FNR Other/NA': [],\n",
    "    'F1 Male': [],\n",
    "    'F1 Female': [],\n",
    "    'F1 Other/NA': []\n",
    "}\n",
    "\n",
    "# Populate the dictionary with the metric values for mitigated and original models\n",
    "groups = ['Male', 'Female', 'Other/NA']  # Adjust these as per your sensitive attribute categories\n",
    "for metric_name in metrics:\n",
    "    for group in groups:\n",
    "        original_metric = metric_frame_original.by_group[metric_name][group].round(3) if group in metric_frame_original.by_group[metric_name] else None\n",
    "        mitigated_metric = metric_frame_mitigated.by_group[metric_name][group].round(3) if group in metric_frame_mitigated.by_group[metric_name] else None\n",
    "        \n",
    "        key_suffix = 'FPR' if metric_name == 'false_positive_rate' else 'FNR' if metric_name == 'false_negative_rate' else 'F1'\n",
    "        metrics_dict[f'{key_suffix} {group}'].append(original_metric)\n",
    "        metrics_dict[f'{key_suffix} {group}'].append(mitigated_metric)\n",
    "\n",
    "# Create the DataFrame\n",
    "metrics_df = pd.DataFrame(metrics_dict, index=['Original', 'Mitigated'])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(metrics_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
